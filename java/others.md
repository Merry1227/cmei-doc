# 防爬问题：
1. 通过cookie记录上个页面的值，后面的调整页面检测该值。
2. 对每个用户，记录当天浏览的详情页数目，限制最大大小。
3. 对header里面浏览器特性和爬虫模拟器进行判断。
4. 对浏览详情页的用户进行记录，比如时间，然后爬虫用户进入详情页的时间超过人工时间，可以防止。
5. 后端产生一段js，前端浏览器执行可产生某个值，后端验证该值。
6. 下载页: 去掉下载，然后格式为图片，加上用户水印。