#Basic Concepts
* Batch Duration: The time interval at which streaming data will be divided into batches* Window:  - window length - The duration of the window (3 in the figure, 3 batch interval). - sliding interval - The interval at which the window operation is performed (2 in the figure, 2 batch invetval ).
 ![spark-streaming](/Users/canhuamei/Desktop/screamshot/sparkstreaming1.png)#RDD Lifecycle in Spark Streaming##In Batch Duration  
* Basically, spark jobs is triggered by timer in spark streaming, and all rdds of  DStream in a batch duration will be cleaned from memory after spark job(action) finish. 
* If we persist one of DSStream in a job (calling DStream.persist), it will be persisted as shown  on Storage on Spark UI, but will be removed later after job is finished automatically.Yon can see from log: BlockManager: Removing RDD xxx
*	There is one workaround to ask spark streaming to hold all rdds generated from different batches by calling ssc.remember. 
eg: we let spark streaming keep all rdds in last 24h by calling ssc.remember(Minutes.apply(60*24))  
From Log: No rdd is removed now during two batch durations.
And on Spark UI, all rdds generated from different batched in the 24h will be shown on Storage:
##In Window

* The rdds generated in successive two windows will be continuously stored in memory or files. 

![spark-streaming-3](/Users/canhuamei/Desktop/screamshot/sparkstreaming3.png
)#Possible ways to control rdd's lifecycle  
Possible ways to control rdd's lifecycle  
 *	Window Operations   rdds in previous batch can be obtained by current batch? Can we build the relations between previous batch and current batch, then union them and generated new DStream ?  http://stackoverflow.com/questions/30048684/spark-streaming-data-sharing-between-batches
	- Conclusion after research:** 
The rdds in successive 2 batches are reserved in the sliding window's lifecycle. It is so depend on window size (i.e. time-dependent)   *	Transform Operation  The transform operation (along with its variations like transformWith) allows arbitrary RDD-to-RDD functions to be applied on a DStream. It can be used to apply any RDD operation that is not exposed in the DStream API. For example, the functionality of joining every batch in a data stream with another dataset is not directly exposed in the DStream API. However, you can easily use transform to do this. This enables very powerful possibilities. For example, one can do real-time data cleaning by joining the input data stream with precomputed spam information (maybe generated with Spark as well) and then filtering based on it.	- Conclusion after research:  
Similar to foreachRDD as bellow.  *	foreachRDD  The most generic output operator that applies a function, func, to each RDD generated from   the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed   in the driver process running the streaming application, and will usually have RDD actions in it   that will force the computation of the streaming RDDs.

	- Conclusion after research:
		- The original rdds generated by spark streaming will be deleted automatically as well by Timer even we try to store the references for each rdds "in spark context scope (programmatically)".  	 	- When I try to not keep references for rdds generated by spark streaming, instead, try to generate new ones  in spark context scope  by passing in the streaming rdds and then union all of them:
	 		- The lifecycle of the new rdds can be out of control of the Timer in spark streaming now, in other word, it can be under control by our program according to our demands when we need persistent them and free them.			- While, the actions which seems be defined in the Spark context scope programmatically actually are still under the control of spark streaming at runtime. i.e., I want the new rdds unions to a global rdd and then record all rdds in one iServer project, but at runtime, no such gobal union rdd will be persisted, the persisted ones are still based on the data in one batch or window.			- The rdds in spark context can be transfer to generated rdds of DStream, but the rdds in DStream can be "returned back" to spark context.

* QueueInputDStream  	Queue of RDDs as a Stream: For testing a Spark Streaming application with test data, one can also create a DStream based on a queue of RDDs, using streamingContext.queueStream(queueOfRDDs). Each RDD pushed into the queue will be treated as a batch of data in the DStream, and processed like a stream. Build a bridge between RDD in SparkContext and DStream SparkStreaming, check rdd and DStream can be converted to each other, and control rdd's lifecycle in SparkCotext.  <https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala>  <https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala>
  	- Conclusion after research: Similar to foreachRDD      - The unionRDD in sparkContext can be programed, but never been triggered even we add some action.
      - The orginalRDD from created by sparkContext can be persisted but can't executed any transformations and actions.
      - The queueRDD from sparkContext can be passed to StreamingContext and transformations predefined on streaming rdd can be executed in each batch interval. #State
<http://www.spark.tc/stateful-spark-streaming-using-transform/>#Starting Process
Starting Process  
  1. Actions(output operations) have to be predefined before context started. Those actions defined after streaming context started will not be never triggered to execute.  
  
```16/06/21 19:17:15 ERROR StreamingContext: Error starting the context, marking it as stoppedjava.lang.IllegalArgumentException: requirement failed: No output operations registered, so nothing to execute
``` 2. After Streamingcontext is started, new inputs,transformations, output operations can't be added by another thread.  

```Exception in thread "Thread-27" java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supported    at org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:220)
```
3. Points to remember (from <http://spark.apache.org/docs/latest/streaming-programming-guide.html->Initializing StreamingContext>):  
*	Once a context has been started, no new streaming computations can be set up or added to it.*	Once a context has been stopped, it cannot be restarted.*	Only one StreamingContext can be active in a JVM at the same time.*	stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.*	A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.#Receiver
Reliable Receiver - A reliable receiver correctly sends acknowledgment to a reliable source when the data has been received and stored in Spark with replication.